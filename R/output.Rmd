---
title: "Image Plotting"
author: "Bonnell, Faltesek, Gates, Lohberger"
date: "8/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# perhaps add another function to name the output dataframe
#' Load all images into one
#'
#' @title Image Loader
#'
#' @param y Folder where images are stored
#'
#'
#' @return returns a dataframe of images
#' @export
#'
#' @examples
#' load_images(here("Images/"))
load_images <- function(y) {
  # x in this case is the name of the directory with the images
  # images would be a great example
  working <- here::here()
  # return file list, full.names allows passage of the entire file paths
  return1 <- list.files(y, full.names = TRUE)
  # prints proof of concept
  print(return1)
  # full paths, if needed
  return2 <- paste(working, return1, sep = "/")
  # assign back to global environment
  images <<- data.frame("local_path" = return1, "global_path" = return2)
}

# perhaps add another function to name the output dataframe
#' Load all images into one
#'
#' @title Convert and Import
#'
#' @param X Folder where images are stored
#'
#'
#' @return returns a dataframe of images
#' @export
#'
#' @examples
#' load_images(here("Images/"))
convert_and_import<-function(x){
  dir.create("converted")
  purrr::map(.x = x$local_path, .f=lower_converter)
  converted_images<<-data.frame(local_path= list.files("converted", full.names = TRUE), old_local_path = images$local_path)
}


convert_and_import<-function(x){
  dir.create("converted")
  purrr::map(.x = x$local_path, .f=lower_converter)
  converted_images<<-data.frame(local_path= list.files("converted", full.names = TRUE), old_local_path = images$local_path)
}

lower_converter <- function(x){
  Z<-magick::image_read(x)
  ZZtop<-magick::image_convert(Z, format = "png", matte = TRUE)
  magick::image_write(ZZtop, paste("converted/",stringi::stri_rand_strings(1, 5, pattern = "[A-Za-z0-9]"),".png", sep = ""), format = "png")
}



# reasonably fast, somewhat annoying to parse through a magick pointer
#' Measure image information and OCR (Optical character recognition)
#'
#' @title Measure Image
#'
#' @param images Image to be read in
#'
#' @details This function returns a unified dataframe that takes your loaded images dataframe and returns a dataframe with image measurements and an OCR reading of the text from the image.
#' @return Returns a dataframe called "measured_images" that is the meta data for the images and an OCR of image text
#' @export
#'
#' @examples
#' measure_images(here("Images/image_1.png"))
measure_images <- function(images) {
  library(magrittr)
 ml_images<-images$local_path%>%
   purrr::map( ~ magick::image_read(.))
measured_images<<-purrr::map_df(1:length(ml_images), ~ data.frame(
  a = .x,
  text = magick::image_ocr(ml_images[[.x]]), 
  info = magick::image_info(ml_images[[.x]])))
  print("OCR results may be misleading if images include no text")
}

# this is horribly slow
#' Get image fluency
#'
#' @title Fluency
#'
#' @param image Image to be read in
#'
#'
#' @return Returns image contrast, similarity, symmetry, complexity
#' @export
#' @details this function implements multiple tests from the Imagefluency package returning a single dataframe, this function can take quite a while to run.
#' @examples
#' fluency(here("Images/image_1.png"))
fluency <- function(image) {
  fl_images <- image$local_path %>%
    purrr::map( ~ imagefluency::img_read(.))
  fluency_results <<- purrr::map_df(1:length(fl_images), ~ data.frame(
    a = .x,
    b = imagefluency::img_contrast(fl_images[[.x]]),
    c = imagefluency::img_self_similarity(fl_images[[.x]]),
    d = imagefluency::img_symmetry(fl_images[[.x]]),
    e = imagefluency::img_complexity(fl_images[[.x]])
  )) 
}


# symmetry function
#' Gets image symmetry
#'
#' @title Image Symmetry
#'
#' @param images Folder where images are stored
#'
#' @details Detects symmetry in an image along three axes.
#' @details Closer to zero means more symmetrical, positive means the image has more ink left or up, negative the opposite
#' @details The diagonal method has defocused areas along the line y=x, the priority is reading symmetry not in the center of the image
#' @return Returns a dataframe with horizontal, vertical, and diagonal symmetry
#' @export
#'
#' @examples
#' symmetry(here("Images/"))
symmetry_analysis <- function(images) {
  ml_images<-images$local_path%>%
    purrr::map( ~ magick::image_read(.))
  symmetry_images<<-purrr::map_df(1:length(ml_images), ~ data.frame(
    a = .x,
    symmetry_lower(ml_images[[.x]])))
  print(symmetry_images)
}


# thirds function
#' Gets image symmetry
#'
#' @title Rule of Thirds
#'
#' @param images Folder where images are stored
#'
#' @details Detects intensity of the use of the thirds on a traditional photographic layout
#' @details Positive means there is more activity in the third versus the full image, negative means less
#' @return Returns a dataframe with scores for each third versus the image
#' @export
#'
#' @examples
#' symmetry(here("Images/"))
thirds_images <- function(images) {
  ml_images<-images$local_path%>%
    purrr::map( ~ magick::image_read(.))
  thirds_results_images<-purrr::map_df(1:length(ml_images), ~ data.frame(
    a = .x,
    low_hor = low1(ml_images[[.x]]),
    high_hor = high1(ml_images[[.x]]),
    left_vert = left1(ml_images[[.x]]),
    right_vert = right1(ml_images[[.x]])))
  thirds_results<-thirds_results_images%>%mutate(vert_focal = ifelse(low_hor>high_hor, "Low", "High"))
  thirds_results<<-thirds_results%>%mutate(hor_focal = ifelse(left_vert>right_vert, "Left", "Right"))
  print(thirds_results)
}


# edge analysis
#' Performs edge analysis
#'
#' @title Edge Analysis
#'
#' @param images Folder where images are stored
#' @details this function uses the same 16 cell grid for image segmentation used in the symmetry function
#' @details underlying math in this function is from a mean of canny edges detected per cell and deviation of them
#' @details the mean would speak to the total value of "ink" in the zone and the deviation may inform the character of the edges
#'
#' @return Returns a dataframe consisting of images, PQ, ST
#' @export
#'
#' @examples
#' load_images(here("Images/"))
edge_analysis <- function(images) {
  ml_images<-images$local_path%>%
    purrr::map( ~ magick::image_read(.))
  edged_R<-purrr::map_df(1:length(ml_images), ~ data.frame(
    a = .x,
    d = edge_lower(ml_images[[.x]])))
  edged_images<<-dplyr::distinct(edged_R, a, .keep_all=TRUE)
  print("edge analysis complete")
}

#' Function to extract image colors
#'
#' @title Image plotter
#'
#' @param X does a thing
#' @details the color class is a function that yields tertiary color regions, see https://en.wikipedia.org/wiki/Tertiary_color
#' @details a segmented version of colors could be available in the next zone
#'
#' @return returns a dataframe of images
#' @export
#'
#' @examples
#' colors() # This is unclear
color_analysis<- function(x){
c_images<-x$local_path
colors_results<-purrr::map_df(.x = c_images, .f=lower_colors)
colors_results<<-colors_results%>%mutate(hue_region = ifelse(mean_blue>mean_red & mean_red >= mean_green, "Violet", ifelse(mean_blue>mean_red & mean_blue < mean_green, "Spring green",
                                                      ifelse(mean_green > mean_red & mean_red >= mean_blue, "Chartreuse", ifelse(mean_green > mean_red & mean_green > mean_blue, "Azure",
                                                      ifelse(mean_red >= mean_green & mean_green >= mean_blue, "Orange", "Rose"))))))
}

#' function that allows you to pass alpha to a GG plot that also encodes other things
#' @description This function is designed to simplify passing arguments into a ggplot with geom_image to produce an image plot
#' @param D is where the data is
#' @param X is the X var
#' @param Y is the Y var
#' @param A is the alpha
imageplot_output <-function(Q,X,Y,A){
  transparent <- function(img) {
    B <- paste(A, "*a", sep = "")
    magick::image_fx(img, expression = B, channel = "alpha")}
  G<-paste("ggplot(",Q, ",aes(",X,",",Y,"))+ggimage::geom_image(image =",Q,"$local_path, image_fun=transparent)", sep = "")
  eval(parse(text=G))}


#completed lower-order functions
low1 <- function(x){
  library(dplyr)
  rudy2 <- magick::image_canny(x)
  ZZZZ <- imager::magick2cimg(rudy2)
  ZZZZZ <- as.data.frame(ZZZZ)
  ZZZZZ <- ZZZZZ %>%
    mutate(color = value * 255)
  # segmentation task
  Q <- max(ZZZZZ$y)
  P <- max(ZZZZZ$x)
  # vert 1
  V1 <- ZZZZZ %>%
    dplyr::filter(x > .16666 * Q & x < .5 * Q)
  V2 <- ZZZZZ %>%
    dplyr::filter(x > .5 * Q & x < Q)
  V3 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .16666 * Q)
  # vert 2
  V4 <- ZZZZZ %>%
    dplyr::filter(x > .5 & x < .83333 * Q)
  V5 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .5 * Q)
  V6 <- ZZZZZ %>%
    dplyr::filter(x > .83333 * Q & x < Q)
  # horz 1
  H1 <- ZZZZZ %>%
    dplyr::filter(y > .16666 * P & y < .5 * P)
  H2 <- ZZZZZ %>%
    dplyr::filter(y > .5 * P & y < P)
  H3 <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .16666 * P)
  # horz 2
  H4 <- ZZZZZ %>%
    dplyr::filter(y > .5 & y < .83333 * P)
  H5 <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .5 * P)
  H6 <- ZZZZZ %>%
    dplyr::filter(y > .83333 * P & y < P)
  L <- mean(H5$value)
  W <- mean(H6$value)
  Q <- mean(H4$value)
  I <- ((Q + Q + Q + W) / 4)
  low_hor <- L - I
  print(low_hor)
  }
high1 <- function(x){
  
  rudy2 <- magick::image_canny(A)
  ZZZZ <- imager::magick2cimg(rudy2)
  ZZZZZ <- as.data.frame(ZZZZ)
  ZZZZZ <- ZZZZZ %>%
    mutate(color = value * 255)
  # segmentation task
  Q <- max(ZZZZZ$y)
  P <- max(ZZZZZ$x)
  # vert 1
  V1 <- ZZZZZ %>%
    dplyr::filter(x > .16666 * Q & x < .5 * Q)
  V2 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < max(x))
  V3 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .16666 * Q)
  # vert 2
  V4 <- ZZZZZ %>%
    dplyr::filter(x > .5 & x < .83333 * Q)
  V5 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .5 * Q)
  V6 <- ZZZZZ %>%
    dplyr::filter(x > .83333 * max(x) & x < max(x))
  # horz 1
  H1 <- ZZZZZ %>%
    dplyr::filter(y > .16666 * P & y < .5 * P)
  H2 <- ZZZZZ %>%
    dplyr::filter(y > .5 * P & y < P)
  H3 <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .16666 * P)
  # horz 2
  H4 <- ZZZZZ %>%
    dplyr::filter(y > .5 & y < .83333 * P)
  H5 <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .5 * P)
  H6 <- ZZZZZ %>%
    dplyr::filter(y > .83333 * P & y < P)
  L <- mean(H1$value)
  W <- mean(H2$value)
  Q <- mean(H3$value)
  I <- ((Q + Q + Q + W) / 4)
  high_hor <- L - I
  print(high_hor)
}
left1 <- function(x){
rudy2 <- magick::image_canny(x)
ZZZZ <- imager::magick2cimg(rudy2)
ZZZZZ <- as.data.frame(ZZZZ)
ZZZZZ <- ZZZZZ %>%
  mutate(color = value * 255)
# segmentation task
Q <- max(ZZZZZ$y)
P <- max(ZZZZZ$x)

# vert 1
V1 <- ZZZZZ %>%
  dplyr::filter(x > .16666 * Q & x < .5 * Q)
V2 <- ZZZZZ %>%
  dplyr::filter(x > .5 * max(x) & x < max(x))
V3 <- ZZZZZ %>%
  dplyr::filter(x > 0 & x < .16666 * Q)
# vert 2
V4 <- ZZZZZ %>%
  dplyr::filter(x > .5 & x < .83333 * Q)
V5 <- ZZZZZ %>%
  dplyr::filter(x > 0 & x < .5 * Q)
V6 <- ZZZZZ %>%
  dplyr::filter(x > .83333 * max(x) & x < max(x))
# horz 1
H1 <- ZZZZZ %>%
  dplyr::filter(y > .16666 * P & y < .5 * P)
H2 <- ZZZZZ %>%
  dplyr::filter(y > .5 * P & y < P)
H3 <- ZZZZZ %>%
  dplyr::filter(y > 0 & y < .16666 * P)
# horz 2
H4 <- ZZZZZ %>%
  dplyr::filter(y > .5 & y < .83333 * P)
H5 <- ZZZZZ %>%
  dplyr::filter(y > 0 & y < .5 * P)
H6 <- ZZZZZ %>%
  dplyr::filter(y > .83333 * P & y < P)


L <- mean(V1$value)
W <- mean(V2$value)
Q <- mean(V3$value)
I <- ((Q + Q + Q + W) / 4)
left_vert <- L - I
print(left_vert)
}
right1 <- function(x){
  rudy2 <- magick::image_canny(x)
  ZZZZ <- imager::magick2cimg(rudy2)
  ZZZZZ <- as.data.frame(ZZZZ)
  ZZZZZ <- ZZZZZ %>%
    mutate(color = value * 255)
  # segmentation task
  Q <- max(ZZZZZ$y)
  P <- max(ZZZZZ$x)
  
  
  # vert 1
  V1 <- ZZZZZ %>%
    dplyr::filter(x > .16666 * Q & x < .5 * Q)
  V2 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < max(x))
  V3 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .16666 * Q)
  # vert 2
  V4 <- ZZZZZ %>%
    dplyr::filter(x > .5 & x < .83333 * Q)
  V5 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .5 * Q)
  V6 <- ZZZZZ %>%
    dplyr::filter(x > .83333 * max(x) & x < max(x))
  # horz 1
  H1 <- ZZZZZ %>%
    dplyr::filter(y > .16666 * P & y < .5 * P)
  H2 <- ZZZZZ %>%
    dplyr::filter(y > .5 * P & y < P)
  H3 <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .16666 * P)
  # horz 2
  H4 <- ZZZZZ %>%
    dplyr::filter(y > .5 & y < .83333 * P)
  H5 <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .5 * P)
  H6 <- ZZZZZ %>%
    dplyr::filter(y > .83333 * P & y < P)
  
  L <- mean(V4$value)
  W <- mean(V5$value)
  Q <- mean(V6$value)
  I <- ((Q + Q + Q + W) / 4)
  right_vert <<- L - I
  print(right_vert)
}

edge_lower<- function(x){
  library(moments)
  rudy2 <- magick::image_canny(x)
  ZZZZ <- imager::magick2cimg(rudy2)
  ZZZZZ <- as.data.frame(ZZZZ)
  dim(ZZZZZ)
  Q <- max(ZZZZZ$y)
  P <- max(ZZZZZ$x)
  # select quarter regions
  # regions start in the upper left and head for bottom right
  
  # select regions
  # TOP ROW
  R1 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .25 * Q) %>%
    dplyr::filter(y > 0 & y < .25 * P)
  
  R2 <- ZZZZZ %>%
    dplyr::filter(x > .25 * Q & x < .5 * Q) %>%
    dplyr::filter(y > 0 & y < .25 * P)
  
  R3 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < .75 * max(x)) %>%
    dplyr::filter(y > 0 & y < .25 * P)
  
  R4 <- ZZZZZ %>%
    dplyr::filter(x > .75 * max(x) & x < max(x)) %>%
    dplyr::filter(y > 0 & y < .25 * P)
  
  # UPPER MIDDLE ROW
  R5 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .25 * Q) %>%
    dplyr::filter(y > .25 & y < .5 * P)
  
  R6 <- ZZZZZ %>%
    dplyr::filter(x > .25 * Q & x < .5 * Q) %>%
    dplyr::filter(y > .25 & y < .5 * P)
  
  R7 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < .75 * max(x)) %>%
    dplyr::filter(y > .25 & y < .5 * P)
  
  R8 <- ZZZZZ %>%
    dplyr::filter(x > .75 * max(x) & x < max(x)) %>%
    dplyr::filter(y > .25 & y < .5 * P)
  
  # LOWER MIDDLE ROW
  R9 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .25 * Q) %>%
    dplyr::filter(y > .5 * P & y < .75 * P)
  
  R10 <- ZZZZZ %>%
    dplyr::filter(x > .25 * Q & x < .5 * Q) %>%
    dplyr::filter(y > .5 * P & y < .75 * P)
  
  R11 <- ZZZZZ %>%
    dplyr::filter(x > .5 * Q & x < .75 * Q) %>%
    dplyr::filter(y > .5 * P & y < .75 * P)
  
  R12 <- ZZZZZ %>%
    dplyr::filter(x > .75 * max(x) & x < max(x)) %>%
    dplyr::filter(y > .5 * P & y < .75 * P)
  
  # bottom row
  R13 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .25 * Q) %>%
    dplyr::filter(y > .75 * P & y < P)
  
  R14 <- ZZZZZ %>%
    dplyr::filter(x > .25 * Q & x < .5 * Q) %>%
    dplyr::filter(y > .75 * P & y < P)
  
  R15 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < .75 * max(x)) %>%
    dplyr::filter(y > .75 * P & y < P)
  
  R16 <- ZZZZZ %>%
    dplyr::filter(x > .75 * max(x) & x < max(x)) %>%
    dplyr::filter(y > .75 * P & y < P)
  
  # sums of each region for canny edge
  R1_edge <- sum(R1$value)
  R2_edge <- sum(R2$value)
  R3_edge <- sum(R3$value)
  R4_edge <- sum(R4$value)
  R5_edge <- sum(R5$value)
  R6_edge <- sum(R6$value)
  R7_edge <- sum(R7$value)
  R8_edge <- sum(R8$value)
  R9_edge <- sum(R9$value)
  R10_edge <- sum(R10$value)
  R11_edge <- sum(R11$value)
  R12_edge <- sum(R12$value)
  R13_edge <- sum(R13$value)
  R14_edge <- sum(R14$value)
  R15_edge <- sum(R15$value)
  R16_edge <- sum(R16$value)
  
  # deviation for each region
  R1_edge_dev <- sd(R1$value)
  R2_edge_dev <- sd(R2$value)
  R3_edge_dev <- sd(R3$value)
  R4_edge_dev <- sd(R4$value)
  R5_edge_dev <- sd(R5$value)
  R6_edge_dev <- sd(R6$value)
  R7_edge_dev <- sd(R7$value)
  R8_edge_dev <- sd(R8$value)
  R9_edge_dev <- sd(R9$value)
  R10_edge_dev <- sd(R10$value)
  R11_edge_dev <- sd(R11$value)
  R12_edge_dev <- sd(R12$value)
  R13_edge_dev <- sd(R13$value)
  R14_edge_dev <- sd(R14$value)
  R15_edge_dev <- sd(R15$value)
  R16_edge_dev <- sd(R16$value)
  
  PQ <- data.frame(
    images, R1_edge, R1_edge_dev,
    R2_edge, R2_edge_dev,
    R3_edge, R3_edge_dev,
    R4_edge, R4_edge_dev,
    R5_edge, R5_edge_dev,
    R6_edge, R6_edge_dev,
    R7_edge, R7_edge_dev,
    R8_edge, R8_edge_dev,
    R9_edge, R9_edge_dev,
    R10_edge, R10_edge_dev,
    R11_edge, R11_edge_dev,
    R12_edge, R12_edge_dev,
    R13_edge, R13_edge_dev,
    R14_edge, R14_edge_dev,
    R15_edge, R15_edge_dev,
    R16_edge, R16_edge_dev
  )
  
  ST <- data.frame(
    kurtosis(R1$value),
    kurtosis(R2$value),
    kurtosis(R3$value),
    kurtosis(R4$value),
    kurtosis(R5$value),
    kurtosis(R6$value),
    kurtosis(R7$value),
    kurtosis(R8$value),
    kurtosis(R9$value),
    kurtosis(R10$value),
    kurtosis(R11$value),
    kurtosis(R12$value),
    kurtosis(R13$value),
    kurtosis(R14$value),
    kurtosis(R15$value),
    kurtosis(R16$value),
    skewness(R1$value),
    skewness(R2$value),
    skewness(R3$value),
    skewness(R4$value),
    skewness(R5$value),
    skewness(R6$value),
    skewness(R7$value),
    skewness(R8$value),
    skewness(R9$value),
    skewness(R10$value),
    skewness(R11$value),
    skewness(R12$value),
    skewness(R13$value),
    skewness(R14$value),
    skewness(R15$value),
    skewness(R16$value)
  )
  edge_results <<- data.frame(images, PQ, ST)
}

symmetry_lower <- function(x) {
  library(dplyr)
  # this routine segments the image into 16 regions and calculates symmetry
  rudy2 <- magick::image_canny(x)
  ZZZZ <- imager::magick2cimg(rudy2)
  ZZZZZ <- as.data.frame(ZZZZ)
  ZZZZZ <- ZZZZZ %>%
    mutate(color = value * 255)
  # segmentation task
  Q <- max(ZZZZZ$y)
  P <- max(ZZZZZ$x)
  
  # y axis symmetry
  top <- ZZZZZ %>%
    dplyr::filter(y > 0 & y < .5 * P)
  
  bottom <- ZZZZZ %>%
    dplyr::filter(y > .5 * P & y == P)
  
  balance <- mean(top$value) - mean(bottom$value)
  horiz <- balance
  sd_top <- sd(top$value)
  sd_bottom <- sd(bottom$value)
  
  # x axis symmetry
  left <- ZZZZZ %>%
    dplyr::filter(x < .5 * max(y))
  
  right <- ZZZZZ %>%
    dplyr::filter(x > .5)
  
  balance <- mean(left$value) - mean(right$value)
  vert <- balance
  sd_left <- sd(left$value)
  sd_right <- sd(right$value)
  
  # TRIANGLE FOLD
  T1 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .5 * Q) %>%
    dplyr::filter(y > 0 & y < .5 * P)
  T2 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .25 * Q) %>%
    dplyr::filter(y > .5 * P & y < .75 * P)
  T3 <- ZZZZZ %>%
    dplyr::filter(x > 0 & x < .125 * Q) %>%
    dplyr::filter(y > .75 * P & y < .875 * P)
  T4 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < .75 * max(x))  %>%
    dplyr::filter(y > 0 & y < .25 * P)
  T5 <- ZZZZZ %>%
    dplyr::filter(x > .75 * max(x) & x < .875 * max(x)) %>%
    dplyr::filter(y > 0 * y & y < .125 * P)
  
  # bottom right big
  T8 <- ZZZZZ %>%
    dplyr::filter(x > .5 * max(x) & x < max(x)) %>%
    dplyr::filter(y > .5 * P & y < P)
  # upper right middle
  T9 <- ZZZZZ %>%
    dplyr::filter(x > .75 * max(x) & x < max(x)) %>%
    dplyr::filter(y < .5 * P & y > .25 * P)
  # upper right small
  T10 <- ZZZZZ %>%
    dplyr::filter(x > .25 * Q & x < .5 * Q) %>%
    dplyr::filter(y > .75 * P & y < P)
  
  T11 <- ZZZZZ %>%
    dplyr::filter(x < .125 * Q & x < .25 * Q) %>%
    dplyr::filter(y > .875 * P & y < P)
  T12 <- ZZZZZ %>%
    dplyr::filter(x > .875 * max(x) & x < max(x)) %>%
    dplyr::filter(y > .125 * P & y < .25 * P)
  
  A <- mean(T1$value) - mean(T8$value)
  B <- mean(T4$value) - mean(T9$value)
  C <- mean(T2$value) - mean(T10$value)
  D <- mean(T3$value) - mean(T11$value)
  E <- mean(T5$value) - mean(T12$value)
  G <- B + C + ((D + E) / 2) / 3
  H <- A + ((B + C) / 2) + ((D + E) / 2) / 3
  
  symmetry <- data.frame(horiz, sd_top, sd_bottom, vert, sd_left, sd_right,
                          central_diagonal = A, corners_diagonal = G,
                          diagonal_overall = H
  )
}

lower_colors <- function(x) {
  loader <- colordistance::loadImage(x, sample.size = 5000)
  plot <- loader$filtered.rgb.2d
  plot2 <- data.frame(plot)
  mean_red <- mean(plot2$r * 255)
  deviation_red <- sd(plot2$r * 255)
  mean_blue <- mean(plot2$b * 255)
  deviation_blue <- sd(plot2$b * 255)
  mean_green <- mean(plot2$g * 255)
  deviation_green <- sd(plot2$g * 255)
  
  # hsv colorset - im skeptical
  
  plot3 <- loader$filtered.hsv.2d
  plot4 <- data.frame(plot3)
  mean_hue <- mean(plot4$h)
  deviation_hue <- sd(plot4$h)
  mean_saturation <- mean(plot4$s)
  deviation_saturation <- sd(plot4$s)
  mean_value <- mean(plot4$v)
  deviation_value <- sd(plot4$v)
  
  # luminance
  luminance <- (mean_red + mean_blue + mean_green) / 3
  # brightness with deviation of brightness
  lum_contrast <- (deviation_red + deviation_blue + deviation_green) / 3

  
  # push to global environment
  colors_results <<- data.frame(
    mean_red, deviation_red, mean_blue, deviation_blue, mean_green, deviation_green,
    mean_hue, deviation_hue, mean_saturation, deviation_saturation, mean_value, deviation_saturation, luminance, lum_contrast
  )
}
```

## Why Image Plotting?

In 2011, the  Software Studies Initiative released a macro known as Image Plot for ImageJ. When combined with a system for measuring the properties of images, the macro allowed users to compose an Image Plot, where the images that they were analyzing would appear themselves as the points of a scatterplot. This is an extremely useful way to analyze visual data as it allows everyone to see the both a meta representation of information about the optical qualities of the images while experiencing them visually. The original documentation focused on patterns of change through an artists collection, for example, how did the works of Mondrain or Van Goh change through the years? 

Underlying much of the rise of Cultural Analytics is the tension between close and distant reading which is often overplayed. The close reading of a particular image will always be an important project, what distant reading methods can provide are: ways of visualizing entire collections, tests for normative claims to clustering or trends, generative documents for encouraging new hypothesis formation. In this sense, the plots produced as image plots appear very much like marginalia in reading or notes taken during field work. 

This particular library grew from work in a lower division undergraduate course at Oregon State University. Our endeavor here is not to provide an entirely new approach, but to bring multiple existing functions into a single commonly used framework that can easily be taught to undergraduates and employed in research by those who are not especially handy with computers. At the same time, in building this project we assume that the future of general education includes some level of data analysis education and that courses in learning common platforms and languages (like tidyverse and Rstudio) will replace standalone GUIs in just a few years. Because our program has links to both communication, art, design, and computer science, our tool set in this package is intended to be useful for 

We designed this package to interface with the tidyverse more generally, all of our measurements are collected in dataframes with one case per row, our tables can easily be joined to make a very wide dataframe. The end plotting logic of this package is geom_image for ggplot2. If you are functional with dplyr and ggplot2, this package should provide you with a comprehensive set of image plotting tools. 


### Importing

For your convenience we have included a selection of images for analysis in this package, they can be called with the function “bernie.” These are twenty images of Bernie Sanders in his mittens from the inauguration, modified by a neural net. 

Our basic importer function takes the name of your folder in quotes and yields a dataframe with each local and global path. This basic loader does not do anything to the images or even read them, it is a simplifier that helps you know where your files actually are. 


```{r load, echo=FALSE}
A<-"/Users/faltesed/Documents/ImagePlottingX/Images/tests/Load20TestImages"
load_images(A)
```

This is always your starting point, which is helpful as it yields an entry in your global environment. It is often helpful to have your files in a single format, which in our case would be PNG. Our function convert_and_import will take the result of your load_images and will convert all of your images to PNG it will put them in a single folder named “converted” in the home directory of your R project. Each image will be assigned a new filename which is a random combination of numbers and letters, which is associated with the original name for that file. 

WARNING: if you run convert and import multiple times, your converted directory will get larger and larger. In accordance with tidy principals our functions are non-destructive. Furthermore, we can imagine use cases where you might chose to convert and land in a single directory for further analysis. If you want to have a clean convert_and_import, delete the directory between function runs. 

You can take a look at some of the individual converted Bernies by opening your “converted” directory, selecting an image with the radio button, and then using the “more” dropdown to open the image in another program. You can see that the images were successfully converted. 


![Alt text] (/Users/faltesed/Documents/ImagePlottingX/Images/delete.png)

```{r load2, echo=TRUE}
convert_and_import(images)
```

### Basic Measurement

Once your images are imported you need to measure them in some way. Your first step in measurement is measure_images, which predictably produces a dataframe called measured_images. This function can tell you many fun things about your images, like what kind of files they are (if you have not converted), their dimensions, color spaces, filesizes, and will OCR any text on the image. This can be very useful for the analysis of memes, our Bernie pictures have no text. 

A second useful measurement method is fluency analysis, which employs the methods from the image fluency package, fielding a dataframe with contrast, self_similarity, symmetry, and complexity. This is a slower function that many others, but can produce really useful results for your analysis. 

```{r measure1}
measure_images(converted_images)
```

MOAR TEXT

#### Your First Plot

```{r measure2}
#for your join
library(dplyr)
library(ggplot2)

#combine the data side by side
mypictures<-bind_cols(converted_images, measured_images)

#the plotting code
imageplot_output("mypictures", "a", "info.filesize", .5)
```

EXPLAIN THIS BAD BOI

## Colors

Colors are great fun and are also very useful. Our color methods are in a single function called color_analysis yielding a dataframe color_results. Under the hood, we are using a number of calculations from both RGB and HSV, depending on your needs. 

Basic Colors
mean_red
Deviation_red
Mean_green
Deviation_green
Mean_blue
Deviation_blue

### Color Theory

To understand what these measure we need to take a detour into some color theory. In many primary schools the primary colors are taught as red, blue, and yellow. For subtractive color and rudimentary paints this then leads to a series of mixing choices and lovely paintings. More advanced versions of this, such as in offset printing, use cyan, magenta, and yellow, often with a layer of black to save money and produce higher contrast line effects, thus CMYK. There are colors where this is not quite as consistent and sharp as we may like which leads to more specific inks, pantones and the like. With light, we are adding colors to produce white. 

Measuring mean red, blue, and green can tell us about how much of that color is present, but that doesn’t mean that the image is that color, what matters are ratios. An all white image would have a lot of every color and no standard deviation. Higher mean values tell you more of what color is present and the deviations show how much variation is present. Consider this image: the top is blue 255 and the bottom is red 255: 

INSERT FIGURE ONE

The values for mean red and blue are nearly directly between the two regions, thus the standard deviation is close to the mean values for red and blue. You will also notice that the values for green are very low as green appears in just some slivers of white on the edges of the image, the standard deviation for the green is also very low as nearly the entire image has no use of green. 

Hue, saturation, and value provide another useful set of measurements for color. We can imagine use cases where you might prefer to use these factors rather than thinking about which raw RGB specifications. For example, it might be helpful to look for trends in saturation or comparing saturation and the use of a certain color, like red. While it would be possible to ask users to calculate their own transformations between RGB and HSV, we simply provide all in the dataframe for speed and convenience. 

#### Color Plotting

```{r colors, message=FALSE}

color_analysis(converted_images)

head(colors_results)

```


MORE TEXT

```{r color2}

mypictures<-bind_cols(mypictures, colors_results)

imageplot_output("mypictures", "mean_red", "mean_blue", .5)

```


##Porportion and Balance

While there was a symmetry method used in the basic section, we have written a few additional functions that work with proportion and balance. Our methods in this section are concerned with finding lines. 



### Symmetry_anaylsis. 

Ten items to report: 
Horizontal symmetry
SD of the top region, SD of the bottom

Vertical symmetry
Sd of the left region, sd of the right region

Central diagonal symmetry along an integral x~y
Central region
Corner region




### Thirds_analysis 

This is our approach for reading images for the use of a standard set of composition trends known as the rule of thirds. Compositions generally position key figures on a tic-tac-toe like grid. This function segments each image into four thirds regions which are then compared for canny edges so that we can compre. NEgative scores mean that there is more activity not on the third, positive scores mean there is more there than the other third. If all scores are very low, it is likely that the image does not conform to the rule of thirds. 

Notice that we have two discrete outputs which tell you which thirds were highest. These are simply there to avoid a calculation for you. In a few cases, the neural net did shift the thirds, meaning that core composition elements did change. 


### Edges_analysis

This is by far our most largest measurement. 

Includes a region by region breakdown of the canny edges detected in each region and the relative standard deviation of the edges in that region. Each region also has a skewness measure and a kurtosis measure, which speak to how much in any given region the distribution leans and how peaky it is. 

The use cases for this particular model would include looking at staks for particular regions where there is a higher density of activity or looking for defocused or empty areas. An approach looking for images that are blue with low edge values in regions 1-4 would be a “sky” detector. If faces were known to be in the images and thirds were established, the scores for R8 and R 12 could be used to look for lead room. There are many ways that you can imagine using this approach to compare particular areas across a set of images, we hope this particular result, while voluminous, is flexible. 

Each represents one-sixteenth of the image numbered in rows from the top left corner, which is R1. The extreme bottom right is R16. 

It is straight forward to write new functions that could approximate symmetry or focus detection 
Note: it is not uncommon for this function to throw NaN when there are regions of the graphic with no edges. This is a drawback of our method. 

### Advanced Analysis Methods




##Outputs and Aesthetics

So far you are familiar with our basic plotting function. This is intended to help folks who are unfamilar create meaningful plots. 

Within the function there are really three distinct things happening: 
A. a function is "passing alpha" to adjust the transparency of your images. 
B. a ggplot2 object is using the standard grammar of graphics approach to build a plot
C. the library ggimage is being used to append the images with modified alpha, which requires a specific assignment of file names

```{r aesthetic1}
library(ggplot2)

#sub-routine A is passing alpha
transparent <- function(img){magick::image_fx(img, expression= ".1*a", channel = "alpha")}

#B is the basic GG plot
ggplot(mypictures, aes(a, info.filesize))+
  #and C is the geom
  ggimage::geom_image(image=mypictures$local_path,image_fun=transparent) + coord_polar()


```


### Styles

Because we are using the gg paradigm for producing output graphics, all of your strategies for using ggplot2 can be used here as well. Because we are using a particular geom, we can't play with many of the options on the front side of the cheatsheet. The back right hand side, is going to be where the fun begins. 

```{r aesthetic2}

#old stuff
transparent <- function(img){magick::image_fx(img, expression= ".1*a", channel = "alpha")}

ggplot(mypictures, aes(a, info.filesize))+
  ggimage::geom_image(image=mypictures$local_path,image_fun=transparent) +
  #newstuff
  #let's make this polar with a linedrawn background and a big title
  coord_polar()+theme_linedraw()+labs(title = "Mittens Are Warm.")

  


```

While aeshtetics aren't perfect here, you can start to see how for things like a symmetry plot or other times when we are looking for outliers a polar plot could be really powerful. 

#### Facets

```{r aesthetic 3}

#old stuff
transparent <- function(img){magick::image_fx(img, expression= ".1*a", channel = "alpha")}

ggplot(mypictures, aes(a, info.filesize))+
  ggimage::geom_image(image=mypictures$local_path,image_fun=transparent) +
  #newstuff
  #no styling, but definitely faceting
  facet_wrap(~a)

  

```





